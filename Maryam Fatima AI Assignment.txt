# -*- coding: utf-8 -*-
"""voicegpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qY4G249zGOyd3UGP3bbplXTw-rjMeBEo
"""

! pip install gradio openai gtts pydub numpy requests groq openai-whisper
!apt-get install -y ffmpeg

import os  # Imports the OS module to interact with the operating system (e.g., file paths, directories, environment variables)
import gradio as gr # Imports Gradio and aliases it as 'gr' to build web-based user interfaces for machine learning models or Python functions
import whisper # Imports OpenAI's Whisper library for automatic speech recognition (ASR) and audio transcription
from gtts import gTTS # Imports the gTTS (Google Text-to-Speech) class to convert text into spoken audio using Google's TTS API
from IPython.display import Audio  # Imports the Audio class to play audio directly within IPython environments like Jupyter notebooks
from groq import Groq # Imports the Groq class, typically used to interact with the Groq API for high-speed AI model inference
import openai  # Imports the OpenAI Python library to access and interact with OpenAI's APIs (e.g., GPT models, Whisper, DALL·E)
import numpy as np # Imports the NumPy library as 'np' for numerical computing, handling arrays, and performing mathematical operations

import os
import gradio as gr
import whisper
from gtts import gTTS
from groq import Groq, GroqError
from typing import Tuple, Union

# Initialize Whisper model # Initialize Whisper model  # Loads the Whisper model for automatic speech recognition (ASR) to transcribe audio into text
model = whisper.load_model("base")

# Initialize Groq API client with the API key directly # Initialize Groq API client with the API key directly  # Sets up the Groq client using your API key to interact with Groq's services for AI model inference

api_key = "groq_api_key" # Sets the API key for authenticating the Groq client
try:
    client = Groq(api_key=api_key)
except Exception as e:  # Initializes the Groq client with the provided API key
    raise RuntimeError(f"Failed to initialize Groq client: {e}")  # Raises a runtime error if Groq client initialization fails

def transcribe_and_respond(audio: str) -> Tuple[str, Union[str, None]]: # Defines a function to transcribe audio and generate a response
    try: # Starts a try block to handle errors within the function
        # Step 1: Transcribe the audio using Whisper # Transcribe the audio using Whisper  # Uses the Whisper model to transcribe the given audio file into text and extracts the transcribed text
        transcription = model.transcribe(audio)
        user_input = transcription['text'] # Extracts the transcribed text from the transcription result to be used as user input


        # Step 2: Generate a response using Groq API and LLaMA model
        try: # Generate a response using Groq API and LLaMA model  # Makes a request to the Groq API using the LLaMA model to generate a response based on user input
            chat_completion = client.chat.completions.create(  # Sends a request to Groq API to generate a chat completion using the LLaMA model
                messages=[{"role": "user", "content": user_input}],
                model="llama3-8b-8192",
            )
            response_text = chat_completion.choices[0].message.content # Extracts the generated response from the API response
        except GroqError as e: # Catches errors specific to the Groq API
            return f"Error in Groq API call: {e}", None  # Returns an error message if there’s an issue with the API call

        # Step 3: Convert the response text to speech using gTTS # Convert the response text to speech using gTTS  # Uses the gTTS library to generate speech from text (response_text) and saves it as an audio file (response.mp3)
        tts = gTTS(response_text)
        audio_path = "response.mp3"
        tts.save(audio_path) # Saves the generated speech as an MP3 file at the specified path (audio_path)

        return response_text, audio_path  # Returns the transcribed text (response_text) and the generated audio file path (audio_path) as outputs for the Gradio interface

    except FileNotFoundError: # Catches the FileNotFoundError exception when a file operation fails due to the file not existing
        return "Error: Audio file not found.", None  # Returns an error message and 'None' if the audio file cannot be found or loaded
    except whisper.WhisperError as e:  # Catches errors specific to the Whisper API (e.g., model loading issues, transcription errors) and stores the exception in variable 'e'
        return f"Error in transcription: {e}", None # Returns a formatted error message with the exception details if there's an issue during transcription
    except Exception as e:  # Catches any other exceptions not previously handled and stores the exception in variable 'e'
        return f"An unexpected error occurred: {e}", None

# Gradio interface for real-time interaction # Gradio interface for real-time interaction  # Sets up a web-based interface with Gradio for users to interact with a model or function in real-time
interface = gr.Interface  # Creates a Gradio interface to connect input elements (like text, images) with a function or model for real-time user interaction
(
    fn=transcribe_and_respond,  # Specifies the function (transcribe_and_respond) to be called when the user interacts with the Gradio interface
    inputs=gr.Audio(type="filepath"),  # Sets the input to accept an audio file (filepath) from the user
    outputs=[gr.Textbox(label="Response"), gr.Audio(label="Voice Response")],  # Defines the output as a textbox for text response and an audio player for the voice response
    live=True  # Enables live updates, meaning the interface responds immediately as the user provides input
)
 
interface.launch()  # Launches the Gradio interface to display and interact with a model or function in a web-based UI